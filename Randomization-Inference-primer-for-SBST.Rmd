---
title: Randomization Inference Primer for SBST
author:
 -name: Nate Higgins
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
    html_document:
      theme: cosmo
      toc: yes
      pandoc_args: [
      "-M", "secPrefix=\\S",
      "--filter", "/usr/local/bin/pandoc-crossref"
      ]
      md_extensions: +autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    pdf_document:
      keep_tex: true
      number_sections: true
      fig_width: 7
      fig_height: 7
      fig_caption: true
      template: bowersarticletemplate.latex
      pandoc_args: [
      "-M", "secPrefix=\\S",
      "--filter", "/usr/local/bin/pandoc-crossref"
      ]
      md_extensions: +autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
...

*Purpose of this document:* To explain what *Randomization Inference
(RI)* is, how it can be used by SBST, and how to use RI to calculate
test statistics (and maybe point estimates).

***1. What Randomization Inference does***

Randomization Inference (RI) is used primarily to evaluate hypotheses.
So… a question: how is it different from regular old hypothesis testing?
Answer: RI doesn’t use distributional assumptions to calculate critical
values. The critical values generated by RI are, in this sense, *exact*,
rather than being generated by a model meant to approximate the data
generating process of a test statistic (what happens in ordinary
hypothesis testing).

So, for instance, RI could be used to test whether or not the mean
$\overset{\bar{}}{x}$ of a bunch of numbers is zero. More generally, RI
can be used to test hypotheses about the value of a statistic $\theta$.

***2. How does Randomization Inference work?***

To begin with, RI is only useful when the data we observe is generated
by a process that involves random assignment -- an experiment. RI uses
the physical act of randomization -- and the fact that the randomization
which we observe *could have gone differently* -- to examine a full set
of alternative values for a statistic $\theta$.

***3. An example to illustrate basic mechanics***

The value of an outcome variable $y$ could have been observed for both
treatment and control units, as in the table below.

  Unit Number   $y_{i}$   $T_{i}$
  ------------- --------- ---------
  1             > 4       0
  2             > 5       0
  3             > 11      0
  4             > 10      0
  5             > 3       0
  6             > 4       1
  7             > 6       1
  8             > 2       1
  9             > 2       1
  10            > 5       1

The first 5 observations are of control units ($T_{i} = 0$) and the
second 5 observations are of treatment units ($T_{i} = 1$). If we want
to test the null hypothesis that the treatment had no effect on $y$, the
usual way to do this would be to compare $\overset{\bar{}}{y_{1}}$to
$\overset{\bar{}}{y_{0}}$, (i.e. evaluate
$\theta = \overset{\bar{}}{y_{1}} - \overset{\bar{}}{y_{0}}$) where
$\overset{\bar{}}{y_{1}}$ represents the mean of $y$ for treatment units
and $\overset{\bar{}}{y_{0}}$ represents the mean of $y$ for control
units. To implement this example in R:

```{r}
  # y values for treatment units

  y1 <- c(4,5,11,10,3)

  n1 <- length(y1)

  # y values for control units

  y0 <- c(4,6,2,2,5)

  n0 <- length(y0)

  # means

  y1.bar <- sum(y1)/n1

  Y0.bar <- sum(y0)/n0
  ---------------------------------
  ---------------------------------

In the above example, $\overset{\bar{}}{y_{1}} = 6.6$ and
$\overset{\bar{}}{y_{0}} = 3.8$. To test $H_{0}:\mu_{1} - \mu_{0} = 0$
(no effect of treatment) using classical statistics, we compare the two
means, $\overset{\bar{}}{y_{1}} - \overset{\bar{}}{y_{0}} = 2.8$,
calculate the standard error of the difference in means (SEDM), and
create a t-statistic. To calculate the SEDM, first calculate the
standard error of $\overset{\bar{}}{y_{1}}$ and
$\overset{\bar{}}{y_{0}}$

$se_{x} = sd_{x}/.$

Or in R:

  -----------------------------------------
  var1 <- sum((y1 - y1.bar)\^2)/(n1-1)

  sd1 <- sqrt(var1)

  se1 <- sd1/sqrt(n1)

  var0 <- sum((y0 - y0.bar)\^2)/(n0-1)

  sd0 <- sqrt(var0)

  se0 <- sd0/sqrt(n0)
  -----------------------------------------
  -----------------------------------------

To evaluate the null hypothesis, we then calculate SEDM as

$SEDM =$

Or in R:

  sedm <- sqrt(se1\^2 + se0\^2)
  ----------------------------------

which yields 1.81659. Take the difference in means of 2.8 and divide by
the SEDM to get the t-statistic of 1.541349. Or, of course, just take a
shortcut and get right to the t-stat with:

  t.test(y1,y0)
  ---------------

However we get here, we do not reject the null hypothesis (at
$\alpha = 0.05$) because the t-stat is less than 2.

Now, how would something similar look using an RI framework? In the RI
framework, we utilize the fact that although the randomization worked
out such that observations 1 through 5 were assigned to treatment and
observations 6 through 10 were assigned to control, this was not the
only way assignment could have gone. It could have, in fact, gone a
whole bunch of other ways. Instead of the treatment group being made up
of observations 1 through 5,

$\{ 1,2,3,4,5\}$,

it could have been made up of observations 1 through 4, plus observation
6 (or 7 or 8 or 9 or 10),

$\{ 1,2,3,4,6\}$

...

$\{ 1,2,3,4,10\}.$

In fact, when separating a dset with 10 observations into two sets of 5,
you are simply creating one group of 5 (the treatment group, say) by
sampling without replacement, with the other group of 5 (the control
group) being the compliment (whatever is not sampled to be a part of the
first group). That is, you are choosing 5 from a group of 10 without
replacement. There are a total of n-choose-k (choose(10,5) = 252)
different randomizations that could have occurred. This is the key to
randomization inference.

***3. Randomization Inference***

To test the null hypothesis of no effect in the RI framework, we don’t
test $H_{0}:\mu_{1} - \mu_{0} = 0$ -- we test the null hypothesis of *no
effect*, i.e. no effect at all on any unit. Testing this null is
different from testing for no effect *on average*. This means that we
test whether or not the data are consistent with a treatment effect of 0
on each unit.

If the effect of treatment on each unit were precisely 0, then the
assignment to treatment was immaterial to generating the $y$ data that
we actually observed. No matter what we did -- no matter which of the
252 possible randomizations we ended up with -- we would have observed
the same $y$ data that we did actually observe (unit 1 would have
produced $y_{1} = 4$, and so on). Our goal now will be to find the value
of the test statistic $\theta$ under each of the 252 possible
assignments to treatment, so that we can see where our test statistic
falls in the distribution. Is it unusual/extreme? Or is it a value in
the heart of the distribution? Let’s find out.

Recall the data as we observed it:

  Unit Number   $y_{i}$   $T_{i}$
  ------------- --------- ---------
  1             > 4       0
  2             > 5       0
  3             > 11      0
  4             > 10      0
  5             > 3       0
  6             > 4       1
  7             > 6       1
  8             > 2       1
  9             > 2       1
  10            > 5       1

If instead the assignment to treatment were exactly reversed, as in

  Unit Number   $y_{i}$   $T_{i}$
  ------------- --------- ---------
  1             > 4       1
  2             > 5       1
  3             > 11      1
  4             > 10      1
  5             > 3       1
  6             > 4       0
  7             > 6       0
  8             > 2       0
  9             > 2       0
  10            > 5       0

then the statistic of interest $\theta$ (difference in means) would have
taken on a value of -2.8 instead of 2.8. If instead the assignment to
treatment were

  Unit Number   $y_{i}$   $T_{i}$
  ------------- --------- ---------
  1             > 4       1
  2             > 5       0
  3             > 11      1
  4             > 10      0
  5             > 3       1
  6             > 4       0
  7             > 6       1
  8             > 2       0
  9             > 2       1
  10            > 5       0

then $\theta$ is equal to 0. What we need is a systematic way to find
each of the possible assignments to treatment, calculate $\theta$ each
time, and save the values of $\theta$ so that we can explore its
distribution. We want to look at each possible assignment of units 1
through 10 to treatment, with the compliment of that set being assigned
to control. To do this, we can use the combinations function in the
gtools package. Invoking combinations(10,5) provides us with a matrix of
all 252 combinations, with each row representing the units assigned to
treatment. For example, the first 10 rows of this matrix would look
like:

\[,1\] \[,2\] \[,3\] \[,4\] \[,5\]

\[1,\] 1 2 3 4 5

\[2,\] 1 2 3 4 6

\[3,\] 1 2 3 4 7

\[4,\] 1 2 3 4 8

\[5,\] 1 2 3 4 9

\[6,\] 1 2 3 4 10

\[7,\] 1 2 3 5 6

\[8,\] 1 2 3 5 7

\[9,\] 1 2 3 5 8

\[10,\] 1 2 3 5 9

We can use this to easily recalculate $\theta$ for each of the 252
observations, storing them as we go.

  -------------------------------------------------------------------
  # Create a vector y containing all outcomes

  y <- c(y1,y0)

  # Create the set of all possible assignments to treatment

  treat <- combinations(10,5)

  # Initialize a vector to store all the thetas

  theta <- rep(0,choose(10,5))

  # Calculate y1.bar for the first assignment

  mean(y\[treat\[1,\]\])

  # Calculate y0.bar for the first assignment

  mean(y\[-treat\[1,\]\])

  # Calculate theta for the first assignment

  mean(y\[treat\[1,\]\]) - mean(y\[-treat\[1,\]\])

  # Now automate that pattern for each of the 252 assignments

  for (i in 1:(choose(10,5))){

  theta\[i\] <- mean(y\[treat\[i,\]\]) - mean(y\[-treat\[i,\]\])

  }
  -------------------------------------------------------------------
  -------------------------------------------------------------------

Done! Now we can look at the vector theta and examine how 2.8 (the
original statistic that we calculated based on the randomization as it
actually happened) compares, i.e. where it falls in the distribution.
It’s probably easiest to first look at data like this in an ordered
fashion.

 ```---------------------------------------------
  # Sort from lowest to highest

  theta <- theta\[order(theta)\]

  # Calculate exact quantile (1/252) for each possible theta

  # How many of 252 are less than or equal to theta\_i?

  idx <- 1:(length(theta))

  quant <- idx/(length(idx))

  # Now find 2.8 on this mapping

  which(theta == 2.8)

  \[1\] 224 225 226 227 228 229 230 231 232 233

  # Note that there are more than 1 (because of ties)

  # Look up the quantiles of theta = 2.8

  quant\[which(theta == 2.8)\]

  \[1\] 0.8888889 0.8928571 0.8968254 0.9007937 0.9047619 0.9087302 \[7\] 0.9126984 0.9166667 0.9206349 0.9246032
 ```---------------------------------------------
 ```---------------------------------------------

So depending on how we deal with ties, anywhere between about 89% and
92% of randomization assignments would have resulted in a theta less
than or equal to 2.8 (the theta we observed), given that the treatment
had no effect. So should we be surprised by a theta of 2.8? Does a theta
of 2.8 provide evidence against the null? Some. If the null is true, 2.8
is unusually large, in the sense that theta is *usually* less than 2.8.

Note that all of this is akin to calculating a p-value for a one-sided
test. To do a two-sided test, we’d look at the absolute values of theta
rather than their actual values.

 ```
  # Calculate absolute values of theta

  abs.theta <- abs(theta)

  # Re-sort from lowest to highest

  abs.theta <- abs.theta\[order(abs.theta)\]

  # Look at two-sided p-value(s)

  quant\[which(abs.theta == 2.8)\]

  \[1\] 0.7738095 0.7777778 0.7817460 0.7857143 0.7896825 0.7936508

  \[7\] 0.7976190 0.8015873 0.8055556 0.8095238 0.8134921 0.8174603

  \[13\] 0.8214286 0.8253968 0.8293651 0.8333333 0.8373016 0.8412698

  \[19\] 0.8452381 0.8492063
 ```
 ```

So, because of ties, there are actually twenty different random
assignments that all lead to a theta of |2.8|. This makes a theta of 2.8
seem a little less unlikely, but it’s still unlikely (at least 77% of
thetas are less than 2.8 when there is no treatment effect).
